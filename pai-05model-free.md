---
layout: page
title: "PAI 5: Model Free RL"
permalink: /pai/model-free
group: pai
---

*Exciting stuffs in this chapter includes DQN, policy gradients/REINFORCE, TRPO/PPO, etc. Current(2023-2025) RLHF are mostly applications of model-free methods.*

First we must formulate the deep reinforcement learning problem. 

**Gradient Update View of TD-learning**(*12.1*)

Recall this update rule of TD-learning: 

$$V^\pi(x) \leftarrow  V^\pi(x) + \alpha_t(r+\gamma V^{\pi} (x') - V^\pi(x))$$

We wish to re-derive it as gradient update of a parameter(for each state). The purpose of providing this view is to (later) expand this gradient update view, and update a NN estimator (of the parameter/V-function) instead (and provide a tractable solution to the problem where we have too many or even continuous states). 

$$V^\pi (x; \mathbf{\theta}) = \theta(x) = \theta_x $$

$$\mathbf{\theta} = [\theta_1...\theta_n]$$

We consider a simple $\frac{1}{2}$-MSE loss and apply the bootstrapping strategy as we did in TD-learning. (Otherwise, it is not possible to update the estimate without knowing the groundtruth, which we don't)

$$\begin{aligned}
& l_\theta(x,r) = \frac{1}{2} (v^\pi(x) - \theta(x)) \\
& = \frac{1}{2} (r + \gamma E_{x\vert x', \pi(x)}[v^\pi(x')] - \theta(x))\\
\\
& \nabla_{\theta(x)} l_\theta(x,r) = \theta(x) - (r + \gamma E_{x\vert x', \pi(x)}[v^\pi(x')])
\end{aligned}$$

After bootstrapping, and a single-sample Monte Carlo Estimate(to avoid transition): 

$$\begin{aligned}
& l = \frac{1}{2} (v^\pi(x) - \theta(x)) \\
& = \frac{1}{2} (r + \gamma \theta^{old}(x') - \theta(x))\\\\
& \delta_{TD} = \theta(x) -  (r + \gamma \theta^{old})\\
\\
& V^\pi(x) = \theta(x) \leftarrow \theta(x) - \alpha_t \delta_{TD}
\end{aligned}$$

Note that in previous chapter, TD-learning converges with infinite iterations. This is requiring infinite(or a lot of) samples as each iteration we acquire/use a new sample(unlike in ML, infinite epoches simply means updating over a dataset for infinite times). This inefficiency is introduced by bootstrapping and single sample Monte Carlo.(*remark 12.1*) This is the cost of fitting a function to something we don't know. 

**Deep RL**

Note that the previous view extends to other variants of TD-learning as well(like Q-learning). Now consider using a linear estimator or even an NN estimator to approximate a Q-function across states(instead of using a single parameter for each state): 

$$Q(x, a; \theta) = \theta^T \phi(x,a)$$

*For Q-learning*, we can define the loss and gradient update(bellman error):

$$\begin{aligned}
& l = \frac{1}{2} (r + \gamma \max_{a'} Q^*(x',a';\theta^{old}) - Q^*(x,a;\theta))\\
\\
& \delta_{B} =  r+\gamma  \max_{a'} Q^*(x',a';\theta^{old}) - Q^*(x,a;\theta)\\
\\
& \theta \leftarrow \theta - \alpha_t \nabla_\theta l(\theta; a,x,r, x') \\
& = \theta - \alpha_t \delta_{B} \nabla_\theta Q^*(x,a;\theta)
\end{aligned}$$

This method with one-hot embeddings of states should be pretty much the same as regular Q-learning. One is welcomed to try. 

However, this vannilla stochastic semi-gradient descent(as opposed to sgd, since we use bootstrapping and single-sample MC) converges very slow. We wish to (introduce some tricks and) alleviate this problem. 

## Heuristics for Value Function Approximation

*Motivation*: stabilize the moving optimization target introduced by boostrapping. 

**DQN: experience replay**

Instead of updating one NN for every observed transition, DQN proposed to maintain two networks: on-line network and target network(with infrequent update for $\theta^{old}$). 

This allows something like a mini-batch update. You would maintain a "replay-buffer"($D$) for new observations. And you perform a mini-batch update (and the optimization target is the same across several updates). 

$$ l = \frac{1}{2} \Sigma_{(x,a,x',r)\in D} (r + \gamma \max_{a'} Q^*(x',a';\theta^{old}) - Q^*(x,a;\theta))$$

- DQN do generates its own data in the pseudo-code. (*But it is off-policy? lol*). See [the reddit discussion](https://www.reddit.com/r/reinforcementlearning/comments/semedp/is_dqn_truly_offpolicy/) on why DQN is off-policy.  
- The online network is updated for each "mini-batch" using this loss. 
- The target network is updated less frequently by copying the weight of online network. (In the pseudo-code: every C steps)
- The new observations is generated by the Q value estimation of the online netwrok.
- PAI didn't mention this: the dataset is initialized to some amount before any updates, and during the inner loop(one iteration is one step in gameplay), you store every new transitions to the dataset. During the mini-batch update, you actually samples random mini-batch from $D$ because "randomizing the samples breaks these (strong) correlations (between consecutive samples) and therefore reduces the variance of the updates". (Mnih et al. 2015)

The DQN paper(Mnih et al. 2015) is on Nature and accessible through purchase or institution. If you do not have access, the pseudo-code is available in page 32 of [this slide](https://courses.grainger.illinois.edu/cs546/sp2018/Slides/Apr05_Minh.pdf) and it should contains the information of interest. 

**Double DQN(DDQN): Overcoming Maximization Bias**

Note that in our previous loss, the maximization operator is applied on the Q value estimated by the target network. This is creates bias because the chosen action might not be the action chosen by the online network (see figure 12.1, which is also from the DDQN paper).

$$ l = \frac{1}{2} \Sigma_{(x,a,x',r)\in D} (r + \gamma \textcolor{red}{\max_{a'} Q^*(x',a';\theta^{old})} - Q^*(x,a;\theta))$$

*If you ask chatGPT/Gemini, it'll tell you that this bias exists because the max operator is not linear and thus:* $E f(X) \neq f EX$

*Also, note that this bias occurs in loss calculation so it is irrelevnt to the playing stage.*

DDQN improved this by specifying the chosen action as the action maximizing the Q-value of the online network (instead of using the max operator). But you still use the Q-value estimation of the target network for parameter update. 

Additionally, PAI provides another on-policy interpretation/implementation of the DQN family: you update the online network for each transition, and update the target network after observing a certain amount of transitions. 

## Policy Approximation/Policy Gradient Methods

*Motivation*: Maximizing over all actions during every update is intractable for large or continuous space, we wish to learn a parameterized policy approximation $\pi_\phi(a \vert x)$ instead. The parameter should maximize the policy value, defined by discounted payoff($G_0$), approximated by bounded discounted payoff($G_{0:T}$). 

**The Optimization Goal**

$$\phi = \argmax J(\pi_\phi); J(\pi) = E_\pi [G_0]=E_\pi [\Sigma \gamma^t R_t] \approx E_\pi[G_{0:T}]$$

Calculating the exact expectation is a bit tricky(and costly): the policy $\pi$ can be stochastic, the unmodeled transition and reward is definitely stochastic. However, we can use MCMC (basically play the game multiple time with the policy, producing multiple episodes) to approximate it:

$$J_T(\phi) \approx \frac{1}{m} \Sigma^m_i g^{i}_{0:T}$$

$$g^i_{0:T} = \Sigma^{T-1}_{t=0} \gamma^t r^i_t$$

<!-- Policy Gradients relies on randomized policy for exploration.  -->

**Policy Gradient**

Then we need to update our policy by some portion of the gradient. 

We first define the possibility of a trajectory($\tau$), and from it, the unbiased gradient estimate. 

$$\Pi_\phi(\tau) = p(x_0) \prod_t \pi_\phi(a_t \vert x_t) p(x_{t+1}\vert x_t, a_t)$$

$$\nabla_\phi J(\phi) \approx \nabla_\phi J_T(\phi) = \nabla_\phi E_{\tau \sim \Pi_\phi} [G_{0:T}]$$

We cannot move the gradient operator inside because the expectation is also dependent on $\phi$. 

We can bypass this headache with regularity assumption by using a **Score Gradient Estimator**

Note that: 

$$ \nabla_\phi \Pi_\phi (\tau) = \nabla_\phi e^{\log\Pi_\phi (\tau)} =\Pi_\phi(\tau) \nabla_\phi \log \Pi_\phi (\tau)$$

The *regularity assumption* states that you can swap the integral and differential operators if (1) the function inside of the integral is differentiable with respect to the parameter (2) the original function and its partial derivative must be integrable. 

Thus, 

$$\begin{aligned}
& \nabla_\phi E_{\tau \sim \Pi_\phi} [G_{0:T}] = \nabla_\phi \int \Pi_\phi(\tau) G_0 d\tau\\
& = \int \nabla_\phi  \Pi_\phi(\tau) G_0 d\tau \\
& = \int G_0 \Pi_\phi(\tau) \nabla_\phi \log \Pi_\phi (\tau) d \tau \\
& = E_{\tau \sim \Pi_\phi} [G_0 \Pi_\phi(\tau) \nabla_\phi \log \Pi_\phi (\tau)]\\
\end{aligned}$$

Now we have moved the derivative inside and we just have to worry about computing $\nabla_\phi \log \Pi_\phi (\tau)$, you can check that(since it's the only parameterized term): 

$$\nabla_\phi \log \Pi_\phi (\tau) = \Sigma_t \nabla_\phi \log \pi_\phi (a_t\vert x_t)$$

Since it's intractable to compute the exact expectation, we apply Monte Carlo sampling: 

$$\nabla_\phi J_T(\phi) \approx \frac{1}{m} \Sigma_i g^{(i)}_{0:T} \Sigma_t \nabla_\phi \log \pi_\phi(a^{(i)}_t\vert x^{(i)}_t)$$

*Note that in MC sampling we always simply take nean rather than weighted average over the sequence likelihood. Because in practice we don't know the $p(x_0)$ or transition probability and thus we can not compute sequence likelihood. We treat every observed sequence equally instead.*

**Controling Variance: baseline and downstream return**

$G_0$ is likely to be a positive numaber(sum of discounted payoff), and our MC sampling can be unstable(variance). We want to control the variance by controling the magnitude of the multiplier by introducing a "normalizing" term(baseline). 

$$E_{\tau \sim \Pi_\phi}[G_0 \nabla_\phi \log \Pi_\phi (\tau)] = E_{\tau \sim \Pi_\phi}[(G_0-b) \nabla_\phi \log \Pi_\phi (\tau)] $$

We can do this and the score gradient remains to be an unbiased estimate because the introduced term is expected to be 0 after expanding. 

$$\begin{aligned}
& = E_{\tau \sim \Pi_\phi}[G_0 \nabla_\phi \log \Pi_\phi (\tau)] - E_{\tau \sim \Pi_\phi}[b \nabla_\phi \log \Pi_\phi (\tau)];\\
&  E_{\tau \sim \Pi_\phi}[b \nabla_\phi \log \Pi_\phi (\tau)] = b \int \nabla_\phi \Pi_\phi(\tau) d\tau \\
& = b \nabla_\phi \int  \Pi_\phi(\tau) d\tau \\
& = b \nabla_\phi 1 = 0
\end{aligned}$$

The baseline $b$ can be a constant. However, it can also be the discounted playoff for previous states. (*Example 12.7*) 

$$G_0 -b(\tau_{0:t-1}) = \gamma^t G_{t:T}$$

**REINFORCE**

This gives us the policy update of the REINFORCE algorithm. (downstream return) Consult *Algorithm 12.8* for pseudo-code. At every time step of an obtained episode, we perform this update: 

$$\phi \leftarrow \phi + \eta \gamma^t g_{t:T} \nabla_\phi \log \pi_\phi(a_t \vert x_t)$$

See book for details on further reducing the variance. 

## On-Policy Actor-Critic

TRPO

PPO

GRPO

## Off-Policy Actor-Critic

Randomized Policies

Reparameterization Trick

## Maximum Entropy RL(MERL)

*I would delay the RLHF part(12.7) of this chapter to "extension".*